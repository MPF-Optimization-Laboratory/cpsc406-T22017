{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification via logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of exercises, you are going to implement logistic regression for email spam detection. We'll use the `Spambase` dataset from UCI Machine Learning Repository, you can find the description of the data from https://archive.ics.uci.edu/ml/datasets/spambase.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data = readdlm(\"spambase.data\", ',')\n",
    "size(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into two sets: a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srand(1234)\n",
    "y = Data[:,end]\n",
    "y = 2*(y - 0.5)\n",
    "X = Data[:,1:(end-1)]\n",
    "n, d = size(X)\n",
    "perm = randperm(n)\n",
    "train_ratio = 0.8\n",
    "n_train = round(Int, n*train_ratio )\n",
    "\n",
    "Xtrain = X[ perm[1:n_train], : ]\n",
    "Xtest = X[ perm[(n_train+1):end], : ]\n",
    "ytrain = y[perm[1:n_train]]\n",
    "ytest = y[ perm[(n_train+1):end] ]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data to have zero mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = mean(Xtrain, 1)\n",
    "sigma = zeros(d)\n",
    "for i = 1:d\n",
    "    sigma[i] = std(X[:,i])\n",
    "    Xtrain[:,i] = (Xtrain[:,i] - mu[i]) / sigma[i]\n",
    "    Xtest[:,i] = (Xtest[:,i] - mu[i]) / sigma[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function for Logistic Regression with 2-norm regularization can be written as\n",
    "$$ f(w) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\log(1 + \\exp ( -y_i w^T x_i ) ). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.\n",
    "Complete the following code that evaluates the objective $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function Obj(Xtrain, ytrain, w, C)\n",
    "    #COMPLETE THE CODE\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "Derive an expression for the gradient $\\nabla f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "Complete the following code that evaluates the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function Grad(Xtrain, ytrain, w, C)\n",
    "    #COMPLETE THE CODE\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Complete the function `gradient_descent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function gradient_descent(f, ∇f, w0, tol=1e-4, stepsize=1, maxIter=1000)\n",
    "\n",
    "    # Initialization\n",
    "    w = copy(w0)\n",
    "    g = g0 = ∇f(w)\n",
    "    norm_g0 = norm(g0)\n",
    "    obj_new = pre_obj = f(w)\n",
    "    iter_list = [0]\n",
    "    obj_list = [pre_obj]\n",
    "    \n",
    "    # Gradient Descent Iteration\n",
    "    iter = 0\n",
    "    while true\n",
    "        \n",
    "        @printf \"it = %3d | f = %10.2e | ||∇f||  = %10.2e\\n\" iter obj_new norm(g)\n",
    "        \n",
    "        # Stop if the norm of the gradient is small relative to its start value\n",
    "        if norm(g) < tol*norm_g0 || iter > maxIter\n",
    "            break\n",
    "        end\n",
    "\n",
    "        iter += 1\n",
    "        \n",
    "        \n",
    "        #COMPLETE BACKTRACK LINE-SEARCH\n",
    "        \n",
    "        \n",
    "        push!(iter_list, iter)\n",
    "        push!(obj_list, obj_new)\n",
    "        \n",
    "    end\n",
    "    return w, iter_list, obj_list    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Train the classifier using $C=1$. Report your test error and plot a figure where xlabel is the number of iteration and ylabel is the objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following code after you complete the functions\n",
    "C = 1\n",
    "f(x) = Obj(Xtrain, ytrain, x, C)\n",
    "∇f(x) = Grad(Xtrain, ytrain, x, C)\n",
    "w0 = zeros(size(Xtrain,2))\n",
    "w, iter_list, obj_list = gradient_descent(f, ∇f, w0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the training error and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = sign.(Xtest*w)\n",
    "@printf \"prediction accuracy on training set: %10.2e \\n\" sum(sign.(Xtrain*w) .== ytrain) / length(ytrain)\n",
    "@printf \"prediction accuracy on testing  set: %10.2e \\n\" sum(ypred .== ytest) / length(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Derive an expression for the Hessian matrix $\\nabla^2 f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Complete the following code that evaluates the Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function Hessian(Xtrain, ytrain, w, C)\n",
    "    #COMPLETE THE CODE\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Complete the function `newton_descent`. Note that we use fixed stepsize. Report your test error and plot a figure where xlabel is the number of iterations and ylabel is the objective value. Does the test error and objective value match with the result from gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_descent(f, ∇f, ∇²f, w0, tol=1e-4, α=1, maxIter = 1000)\n",
    "    \n",
    "    #Initialization\n",
    "    w = copy(w0)\n",
    "    g = g0 = ∇f(w)\n",
    "    H = ∇²f(w)\n",
    "    norm_g0 = norm(g0)\n",
    "    obj_value = f(w)\n",
    "    iter_list = [0]\n",
    "    obj_list = [obj_value]\n",
    "    \n",
    "    iter = 0   \n",
    "    while true\n",
    "        \n",
    "        @printf \"it = %3d | f = %10.2e | ||∇f||  = %10.2e\\n\" iter obj_value norm(g)\n",
    "        \n",
    "        # Stop if the norm of the gradient is small relative to its start value\n",
    "        if norm(g) < tol*norm_g0 || iter > maxIter\n",
    "            break\n",
    "        end\n",
    "\n",
    "        iter += 1\n",
    "        \n",
    "        \n",
    "        #COMPLETE THE UPDATE RULE\n",
    "        \n",
    "        \n",
    "        obj_value = f(w)\n",
    "        \n",
    "        push!(iter_list, iter)\n",
    "        push!(obj_list, obj_value)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return w, iter_list, obj_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code after you complete the functions\n",
    "C = 1\n",
    "f(x) = Obj(Xtrain, ytrain, x, C)\n",
    "∇f(x) = Grad(Xtrain, ytrain, x, C)\n",
    "hessian(x) = Hessian(Xtrain, ytrain, x, C)\n",
    "w0 = zeros(size(Xtrain,2))\n",
    "w, iter_list, obj_list = newton_descent(f, ∇f, hessian, w0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the training error and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = sign.(Xtest*w);\n",
    "@printf \"prediction accuracy on training set: %10.2e \\n\" sum(sign.(Xtrain*w) .== ytrain) / length(ytrain)\n",
    "@printf \"prediction accuracy on testing  set: %10.2e \\n\" sum(ypred .== ytest) / length(ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
